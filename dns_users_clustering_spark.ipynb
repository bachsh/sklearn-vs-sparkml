{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Clustering Pipeline Implementation\n",
    "\n",
    "We now recreate the clustering pipeline using Spark. We will do so in two ways:\n",
    "* Using SparkML\n",
    "* By importing the model we trained in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains dependency imports in Ivy (similar to Maven/Gradle).\n",
    "Note that we use the <a href=\"https://github.com/jpmml/jpmml-evaluator-spark\">jpmml-evaluator-spark</a> package for loading the model trained in Python. An older version of pmml-model is included in SparkML which prevents us from using newer PMML versions. The clean way to address that is to shade it in the way described <a href=\"https://github.com/jpmml/jpmml-sparkml#library\">here</a>. In our case we just import the newer version of pmml-model before importing SparkML, and it solves the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This cell will generate a large output on the first run (it needs to download the dependencies)\n",
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jpmml:pmml-model:1.3.9`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`org.jpmml:jpmml-evaluator-spark:1.1.0`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to initialize the SparkSession. This object is the way we load data to Spark and perform transformations and actions on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3fa6f2f0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .appName(\"Spark clustering demo app\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataset\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, count_hour0: string ... 7 more fields]\n",
       "\u001b[36morganizations\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = sparkSession.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\").option(\"delimiter\", \"\\t\")\n",
    "    .load(\"user_dataset_sample.tsv\")\n",
    "\n",
    "val organizations = sparkSession.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"organizations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_ip: string (nullable = true)\n",
      " |-- count_hour0: string (nullable = true)\n",
      " |-- count_hour1: string (nullable = true)\n",
      " |-- count_hour2: string (nullable = true)\n",
      " |-- count_hour3: string (nullable = true)\n",
      " |-- count_hour4: string (nullable = true)\n",
      " |-- count_hour5: string (nullable = true)\n",
      " |-- domain_count: string (nullable = true)\n",
      " |-- nxdomain_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.DoubleType\n",
       "\u001b[39m\n",
       "\u001b[36mdatasetWithQueryCount\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, count_hour0: string ... 8 more fields]\n",
       "\u001b[36mdatasetWithNorms\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [user_ip: string, count_hour0: string ... 14 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "val datasetWithQueryCount = dataset\n",
    "    .na.replace(dataset.columns, Map(\"NULL\" -> \"0\"))\n",
    "    .withColumn(\"domain_count\", $\"domain_count\".cast(DoubleType))\n",
    "    .withColumn(\"nxdomain_count\", $\"nxdomain_count\".cast(DoubleType))\n",
    "//     .withColumn(\"query_count\", windowColumns.map(x => dataset(x)).reduce(_+_))\n",
    "    .withColumn(\"query_count\", 'count_hour0 + 'count_hour1 + 'count_hour2 + 'count_hour3 + 'count_hour4 + 'count_hour5)\n",
    "val datasetWithNorms = (0 to 5).foldLeft(datasetWithQueryCount)(\n",
    "    (df, window) =>\n",
    "        df.withColumn(s\"count_hour${window}_norm\", df(s\"count_hour$window\")/df(\"query_count\"))\n",
    "    ).filter($\"query_count\".between(20, 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Dataframe for SparkML objects\n",
    "While sklearn objects act on matrices, SparkML objects act on a `Vector` field in a dataframe which contains the features, and a label field (for supervised learning models). Transforming our dataframe to the desired format is a bit tedious but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_ip    |features                                                                                                                                |\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0.102.79 |[9303.0,2456.0,0.010856712888315597,0.09061593034504999,0.19219606578523057,0.19703321509190583,0.27152531441470495,0.23777276147479307]|\n",
      "|0.0.129.25 |[2930.0,1188.0,0.07508532423208192,0.2955631399317406,0.089419795221843,0.12116040955631399,0.16655290102389078,0.2522184300341297]     |\n",
      "|0.0.144.177|[662.0,36.0,0.11027190332326284,0.2190332326283988,0.20996978851963746,0.16314199395770393,0.1148036253776435,0.18277945619335348]      |\n",
      "|0.0.167.184|[554.0,85.0,0.018050541516245487,0.02707581227436823,0.20577617328519857,0.05415162454873646,0.2815884476534296,0.41335740072202165]    |\n",
      "|0.0.177.228|[4737.0,661.0,0.10724086974878615,0.25965801139962,0.16846105129829006,0.17669411019632678,0.16086130462317924,0.12708465273379776]     |\n",
      "|0.0.194.14 |[590.0,21.0,0.01864406779661017,0.01864406779661017,0.8406779661016949,0.10847457627118644,0.005084745762711864,0.00847457627118644]    |\n",
      "|0.0.201.255|[471.0,81.0,0.006369426751592357,0.31210191082802546,0.15498938428874734,0.31422505307855625,0.1740976645435244,0.03821656050955414]    |\n",
      "|0.0.209.160|[353.0,178.0,0.29178470254957506,0.06515580736543909,0.28611898016997167,0.019830028328611898,0.014164305949008499,0.32294617563739375] |\n",
      "|0.0.211.128|[12601.0,2179.0,0.010475359098484247,0.13983017220855487,0.16649472264106024,0.06285215459090548,0.2306959765098008,0.3896516149511943] |\n",
      "|0.0.242.250|[5543.0,614.0,0.12231643514342413,0.22532924409164712,0.24246797762944253,0.15821757171206927,0.1704852967707018,0.08118347465271514]   |\n",
      "|0.0.247.21 |[261.0,74.0,0.011494252873563218,0.09195402298850575,0.6513409961685823,0.0038314176245210726,0.2260536398467433,0.01532567049808429]   |\n",
      "|0.0.253.242|[183.0,64.0,0.0,0.16939890710382513,0.5136612021857924,0.10382513661202186,0.1912568306010929,0.02185792349726776]                      |\n",
      "|0.0.28.93  |[1925.0,680.0,0.10701298701298702,0.2535064935064935,0.0,0.0,0.44,0.19948051948051948]                                                  |\n",
      "|0.0.33.70  |[1371.0,349.0,0.0014587892049598833,0.04011670313639679,0.6907366885485048,0.2224653537563822,0.0437636761487965,0.0014587892049598833] |\n",
      "|0.0.46.180 |[1322.0,451.0,0.08169440242057488,0.28895612708018154,0.22692889561270801,0.2004538577912254,0.08018154311649017,0.12178517397881997]   |\n",
      "|0.0.84.184 |[187.0,469.0,0.20320855614973263,0.08021390374331551,0.03208556149732621,0.0213903743315508,0.6149732620320856,0.0481283422459893]      |\n",
      "|0.1.11.252 |[157.0,118.0,0.01910828025477707,0.2611464968152866,0.12738853503184713,0.006369426751592357,0.025477707006369428,0.5605095541401274]   |\n",
      "|0.1.125.233|[576.0,167.0,0.10243055555555555,0.17708333333333334,0.0,0.11631944444444445,0.15625,0.4479166666666667]                                |\n",
      "|0.1.131.75 |[2106.0,665.0,0.020417853751187084,0.38746438746438744,0.26875593542260207,0.15147198480531815,0.05887939221272555,0.11301044634377967] |\n",
      "|0.1.169.156|[198.0,75.0,0.07575757575757576,0.08585858585858586,0.005050505050505051,0.04040404040404041,0.41919191919191917,0.37373737373737376]   |\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.VectorAssembler\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mselectedFeatures\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"query_count\"\u001b[39m,\n",
       "  \u001b[32m\"domain_count\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour0_norm\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour1_norm\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour2_norm\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour3_norm\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour4_norm\"\u001b[39m,\n",
       "  \u001b[32m\"count_hour5_norm\"\u001b[39m\n",
       ")\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mVectorAssembler\u001b[39m = vecAssembler_1d9548d9181b\n",
       "\u001b[36mfeaturesDataset\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, features: vector]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val selectedFeatures = Array(\"query_count\", \"domain_count\") ++\n",
    "    (0 to 5).map(x => s\"count_hour${x}_norm\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(selectedFeatures)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val featuresDataset = assembler.transform(datasetWithNorms).select('user_ip, 'features)\n",
    "\n",
    "featuresDataset.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.StandardScaler\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.clustering.KMeans\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mscaler\u001b[39m: \u001b[32mStandardScaler\u001b[39m = stdScal_3626ad1ed1ca\n",
       "\u001b[36mclusterer\u001b[39m: \u001b[32mKMeans\u001b[39m = kmeans_66b9ddc9a2d9\n",
       "\u001b[36mscalerModel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mStandardScalerModel\u001b[39m = stdScal_3626ad1ed1ca\n",
       "\u001b[36mscaledFeaturesDataset\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, features: vector ... 1 more field]\n",
       "\u001b[36mclusteringModel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclustering\u001b[39m.\u001b[32mKMeansModel\u001b[39m = kmeans_66b9ddc9a2d9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "    .setInputCol(assembler.getOutputCol)\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "val clusterer = new KMeans()\n",
    "    .setK(2).setSeed(42L)\n",
    "    .setFeaturesCol(scaler.getOutputCol)\n",
    "\n",
    "val scalerModel = scaler.fit(featuresDataset)\n",
    "val scaledFeaturesDataset = scalerModel.transform(featuresDataset)\n",
    "\n",
    "// Fitting the model takes about a minute\n",
    "val clusteringModel = clusterer.fit(scaledFeaturesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_ip: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatasetWithClusters\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, features: vector ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datasetWithClusters = clusteringModel.transform(scaledFeaturesDataset)\n",
    "datasetWithClusters.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the results to a file. Note that the file save fails if the file already exists. You can use `SaveMode.Overwrite` to force overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetWithClusters.select('user_ip, 'prediction).write.format(\"csv\").save(\"clusters_sparkml.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sizes of the clusters we are getting. We expect the sizes to be comparable (not necessarily identical) to the ones we got in the Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction|count |\n",
      "+----------+------+\n",
      "|1         |67553 |\n",
      "|0         |104816|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetWithClusters.groupBy('prediction).count.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Imported from Scikit-Learn (PMML)\n",
    "We now read the pmml file exported by the Python part. The file contains a pipeline with the scaler and the clustering part. We expect that running the model on the same data will result in the same partition to clusters, this can be checked on the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.jpmml.evaluator.spark.{EvaluatorUtil, TransformerBuilder}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.jpmml.evaluator.spark.{EvaluatorUtil, TransformerBuilder}\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpmmlFile\u001b[39m: \u001b[32mFile\u001b[39m = clustering_pipeline.pmml\n",
       "\u001b[36mevaluator\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjpmml\u001b[39m.\u001b[32mevaluator\u001b[39m.\u001b[32mEvaluator\u001b[39m = org.jpmml.evaluator.clustering.ClusteringModelEvaluator@2f082cf0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pmmlFile = new File(\"clustering_pipeline.pmml\")\n",
    "val evaluator = EvaluatorUtil.createEvaluator(pmmlFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpmmlTransformerBuilder\u001b[39m: \u001b[32mTransformerBuilder\u001b[39m = org.jpmml.evaluator.spark.TransformerBuilder@a6c0bed\n",
       "\u001b[36mpmmlTransformer\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mTransformer\u001b[39m = pmml-transformer\n",
       "\u001b[36mdatasetWithPmmlClusters\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, count_hour0: string ... 15 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pmmlTransformerBuilder = new TransformerBuilder(evaluator)\n",
    "//     .withTargetCols()\n",
    "    .withOutputCols()\n",
    "    .withLabelCol(\"prediction\")\n",
    "    .exploded(false)\n",
    "\n",
    "val pmmlTransformer = pmmlTransformerBuilder.build()\n",
    "val datasetWithPmmlClusters = pmmlTransformer.transform(datasetWithNorms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_ip: string (nullable = true)\n",
      " |-- count_hour0: string (nullable = true)\n",
      " |-- count_hour1: string (nullable = true)\n",
      " |-- count_hour2: string (nullable = true)\n",
      " |-- count_hour3: string (nullable = true)\n",
      " |-- count_hour4: string (nullable = true)\n",
      " |-- count_hour5: string (nullable = true)\n",
      " |-- domain_count: double (nullable = true)\n",
      " |-- nxdomain_count: double (nullable = true)\n",
      " |-- query_count: double (nullable = true)\n",
      " |-- count_hour0_norm: double (nullable = true)\n",
      " |-- count_hour1_norm: double (nullable = true)\n",
      " |-- count_hour2_norm: double (nullable = true)\n",
      " |-- count_hour3_norm: double (nullable = true)\n",
      " |-- count_hour4_norm: double (nullable = true)\n",
      " |-- count_hour5_norm: double (nullable = true)\n",
      " |-- pmml: struct (nullable = true)\n",
      " |    |-- Cluster: string (nullable = false)\n",
      " |    |-- affinity(0): double (nullable = false)\n",
      " |    |-- affinity(1): double (nullable = false)\n",
      " |    |-- prediction: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetWithPmmlClusters.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mclusters\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_ip: string, Cluster: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clusters = datasetWithPmmlClusters.select(\"user_ip\", \"pmml.Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters.write.format(\"csv\").save(\"clusters_pmml.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare this file with the one written by the Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkSession.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
